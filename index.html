<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AirExo">
  <meta name="keywords" content="ExcavatorRL, Robotics, Robot Learning, Exoskeletons">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ExcavatorRL: A Novel Benchmark and Environment for Reinforcement Learning in Excavation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://airexo.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://graspnet.net/anygrasp.html">
            AnyGrasp
          </a>
          <a class="navbar-item" href="https://rh20t.github.io/">
            RH20T
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>
-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ExcavatorRL: A Novel Benchmark and Environment for Reinforcement Learning in Excavation</h1>
          <!-- 
          <h4 class="title is-4 conference">ICRA 2024</h4>
          -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="1025034345@sjtu.edu.cn">Qianyou Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="gaol49@sany.com.cn">Le Gao</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="wu_duidi@sjtu.edu.cn">Duidi Wu</a><sup>1</sup>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="martin123454412@gmail.com">Yihao Lei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="401614187@qq.com">Xinyao Meng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="wanglingyu_025@sjtu.edu.cn">Lingyu Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="jinhuaqj@sjtu.edu.cn">Jin Qi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="hujie@sjtu.edu.cn">Jie Hu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <sup>1</sup><span class="author-block">School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China</span><br>
            <sup>2</sup><span class="author-block">School of Design, Shanghai Jiao Tong University, Shanghai, 200240, China </span><br>
            <sup>3</sup><span class="author-block">Sany heavy machinery Co.ltd, Kunshan, 215300, Jiangsu, China </span>
          </div>
          <!-- 
          <div class="is-size-5 publication-authors">
            <span class="author-block">(* = Equal Contribution, <a href="mailto:galaxies@sjtu.edu.cn">galaxies@sjtu.edu.cn</a>, <a href="mailto:fhaoshu@gmail.com">fhaoshu@gmail.com</a>, <a href="mailto:sommerfeld@sjtu.edu.cn">sommerfeld@sjtu.edu.cn</a>)</span>
          </div>
          -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2309.14975.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!--
              <span class="link-block">
                <a href="static/pdfs/poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#airexo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-robot"></i>
                  </span>
                  <span>Sample Demonstration Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/video1.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ExcavatorRL</span> aims to improve excavation automation by applying a reinforcement learning framework that utilizes real-world data and simulations for informed decision-making.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="abstract">Abstract</a></h2>
        <div class="content has-text-justified">
          <p>
            The construction machinery industry is growing rapidly due to automation and artificial intelligence.
             Excavator technology is at the forefront of this change. Excavator automation has huge potential,
              but must deal with the complexity of changing and unpredictable setups and the need for precise handling. Here, we present <b><i>ExcavatorRL</i></b>,
               an innovative reinforcement learning framework and system designed to overcome the challenges.
                Our research utilizes empirical data collected from real-world scenarios and a Unity-based simulation for the training and evaluation of reinforcement learning models.
                 <b><i>ExcavatorRL</i></b> has enhanced its learning capabilities by generating an augmented dataset comprising 155 trajectories using DoppelGANger.
                  It also integrates terrain data into the decision-making process by using grid feature vectors processed by CLIP.
                   This method forms a comprehensive state, action, and reward framework essential for effective learning in intricate settings. Using the Decision Transformer algorithm,
                    our proposed system achieves the highest average full bucket rate of 0.83 and a cosine similarity of 0.94 in terrain shape assessment.
                     The results outperform other systems and standard manual trajectories.
                      <b><i>ExcavatorRL</i></b> underscores the potential of combining augmented datasets with detailed terrain features to enhance the efficiency and reliability of excavation automation.
                       Future work will be directed towards improving the simulation environment, integrating advanced RL algorithms, and conducting real-world deployment tests.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<!--
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="ExcavatorRL">ExcavatorRL</a></h2>
        </div>
        
        <div class="content has-text-justified">
          <div class="column">
          <img src="static/images/figure0.png"></img>
          </div>
          <p>
            In <b><i>ExcavatorRL</i></b>, we initially construct an Excavation Scene, which details the data collection process from real-world scenarios and the development 
            of a Unity-based simulation environment. We then conduct an analysis of a typical excavation workflow based on the requirements of real-world excavation scenarios 
            and accordingly define the excavation task. Subsequently, we implement Data Enhancement techniques to improve the quality and diversity of the training data. We train
             a Generative Adversarial Network (GAN) capable of generating potential operational trajectories derived from the actions of the excavator within the simulation scene.
              These trajectories are subsequently refined through a Trajectory Screening module to ensure their feasibility and safety. Within the RL framework, we define both the
               reward space and the action space to quantify the performance of the excavator in the simulated scene. We process complex terrain into grid-like pseudo-images and convert
                them into grid feature vectors using the image encoder in large model CLIP \cite{radford2021learning}. This approach encodes the excavator's motion parameters and terrain
                 information into state space to support decision making. Finally, we utilize popular RL algorithm, the Decision Transformer (DT) , to train, infer, and evaluate ExcavatorRL.
                  Through a series of experiments and evaluations, we demonstrate the efficacy of the proposed system in achieving unmanned excavation. 
          </p>
          <div class="column">
          <video id="airexo" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/airexo1.mp4"
                    type="video/mp4">
          </video>
          </div>
          <p>
            After calibration with a dual-arm robot, <b><i>AirExo</i></b> can achieve precise joint-level teleoperations of the robot for teleoperated demonstration collection. 
          </p>     
          <div class="column has-text-centered">
            <video id="airexo" autoplay muted loop playsinline>
              <source src="./static/videos/airexo2.mp4"
                      type="video/mp4">
            </video>
            </div>
          <p>
            Moreover, contributed to its portable property, <b><i>AirExo</i></b> enables <i>in-the-wild data collection for dexterous manipulation without needing a robot</i>. Humans can wear <b><i>AirExo</i></b>, conduct manipulation in the wild, and collect demonstrations at scale. The one-to-one joint mapping also reduces the barriers of transferring policies trained on human-collected data to robots. 
          </p>          
          <div class="column has-text-centered">
            <video width="220" id="airexo" autoplay muted loop playsinline>
              <source src="./static/videos/airexo3.mp4"
                      type="video/mp4">
            </video>
            </div>
          <p>
            This breakthrough capability not only simplifies data collection but also extends the reach of whole-arm manipulation into unstructured environments, where robots can learn and adapt from human interactions. In the future, we are excited to see our <b><i>AirExo</i></b> collecting large-scale demonstrations in unstructured environments and facilitating robot learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="learning-itw">ExcavatorRL</a></h2>
        <div class="content has-text-justified">
          <div class="column">
          <img src="static/images/figure0.png"></img>
          </div>
          <p>
            In <b><i>ExcavatorRL</i></b>, we initially construct an Excavation Scene, which details the data collection process from real-world scenarios and the development 
            of a Unity-based simulation environment. We then conduct an analysis of a typical excavation workflow based on the requirements of real-world excavation scenarios 
            and accordingly define the excavation task. Subsequently, we implement Data Enhancement techniques to improve the quality and diversity of the training data. We train
             a Generative Adversarial Network (GAN) capable of generating potential operational trajectories derived from the actions of the excavator within the simulation scene.
              These trajectories are subsequently refined through a Trajectory Screening module to ensure their feasibility and safety. Within the RL framework, we define both the
               reward space and the action space to quantify the performance of the excavator in the simulated scene. We process complex terrain into grid-like pseudo-images and convert
                them into grid feature vectors using the image encoder in large model CLIP \cite{radford2021learning}. This approach encodes the excavator's motion parameters and terrain
                 information into state space to support decision making. Finally, we utilize popular RL algorithm, the Decision Transformer (DT) , to train, infer, and evaluate ExcavatorRL.
                  Through a series of experiments and evaluations, we demonstrate the efficacy of the proposed system in achieving unmanned excavation. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="learning-itw">Learning in the Wild</a></h2>
        <div class="column">
              <!-- Data Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1f_bmrFPep90aUSBj28TdXRiNvHo7PpxR?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Sample Demonstration Data</span>
                </a>
              </span>              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AirExo/act-in-the-wild"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
        </div>
        <div class="content has-text-justified">
          <div class="column">
          <img src="static/images/learning-itw.png"></img>
          </div>
          <p>
            <b><i>AirExo</i></b> serves as a natural bridge for the kinematic gap between humans and robots. To address the domain gap between images, our approach involves a two-stage training process. In the first stage, we pre-train the policy using in-the-wild human demonstrations and actions recorded by the exoskeleton encoders. During this phase, the policy primarily learns the high-level task execution strategy from the large-scale and diverse in-the-wild human demonstrations. Subsequently, in the second stage, the policy undergoes fine-tuning using teleoperated demonstrations with robot actions to refine the motions based on the previously acquired high-level task execution strategy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="experiments">Experimental Results</a></h2>
        <div class="content has-text-justified">
          <div class="column">
            <video id="result1" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/result1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <p>
            We evaluate the performance of different methods on  the "Gather Balls" task. After applying our in-the-wild learning framework, with the assistance of in-the-wild demonstrations, ACT can achieve the same level of performance as 50 teleoperated demonstrations with just 10 teleoperated demonstrations. This demonstrates that our learning framework with in-the-wild demonstrations makes the policy more sample-efficient for teleoperated demonstrations.
          </p>
          <p>
            <details>
              <summary style="color:slateblue">&nbsp;<b>Links to  full videos in this experiment.</b></summary>
              <br>
            <table style="text-align:center">
              <tr><td><b># Teleoperated Demonstrations</b></td> <td><b># In-the-Wild Demonstrations</b></td><td><b>Method</b></td> <td><b>Video</b></td></tr>
              <tr></tr>
              <tr>
                <td>50</td><td>-</td><td>VIP + NN</td> <td><a href="https://youtu.be/HTcxeEhi5p4">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>VC-1 + NN</td> <td><a href="https://youtu.be/jDO4wiU-3ow">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>MVP + NN</td> <td><a href="https://youtu.be/w3gp-_cFVrQ">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>VINN</td> <td><a href="https://youtu.be/CVMD41ZGvMI">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>ConvMLP</td> <td><a href="https://youtu.be/oVjgKlvzG34">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>BeT</td> <td><a href="https://youtu.be/H6SsaK_f16k">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>ACT</td> <td><a href="https://youtu.be/_AwjfN4wgxo">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>-</td><td>VINN</td> <td><a href="https://youtu.be/GS9mbfLY-lA">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>-</td><td>ACT</td> <td><a href="https://youtu.be/H6SsaK_f16k">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>50</td><td>ACT</td> <td><a href="https://youtu.be/_I-RvZPCYSg">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>100</td><td>ACT</td> <td><a href="https://youtu.be/nYQoMoWeTdk">Link</a></td>
              </tr>
              </table>
            </details>
          </p>
          <div class="column">
            <video id="result2" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/result2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <p>
            We also evaluate the performance of different methods on the "Grasp from the Curtained Shelf" task. After training with our in-the-wild learning framework, ACT exhibits a significant improvement in success rates in the "grasp" and "throw" stages. It achieves even higher success rates, surpassing those obtained with the original set of 50 teleoperated demonstrations lasting more than 20 minutes, using only 10 such demonstrations lasting approximately 3 minutes.  This highlights that our proposed in-the-wild framework indeed enables the policy to learn a better strategy, effectively enhancing the success rates in the later stages of multi-stage tasks.
          </p>
          <p>
            <details>
              <summary style="color:slateblue">&nbsp;<b>Links to  full videos in this experiment.</b></summary>
              <br>
            <table style="text-align:center">
              <tr><td><b># Teleoperated Demonstrations</b></td> <td><b># In-the-Wild Demonstrations</b></td><td><b>Method</b></td> <td><b>Video</b></td></tr>
              <tr></tr>
              <tr>
                <td>50</td><td>-</td><td>VINN</td> <td><a href="https://youtu.be/ddWwnBy4Iw4">Link</a></td>
              </tr>
              <tr>
                <td>50</td><td>-</td><td>ACT</td> <td><a href="https://youtu.be/R-X5ZXFpg7Q">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>-</td><td>VINN</td> <td><a href="https://youtu.be/8_lM1SaUma0">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>-</td><td>ACT</td> <td><a href="https://youtu.be/x8nTK02A0Cw">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>50</td><td>ACT</td> <td><a href="https://youtu.be/beUuG9tUM50">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>100</td><td>ACT</td> <td><a href="https://youtu.be/NW9T2SeH0Uk">Link</a></td>
              </tr>
              </table>
            </details>
            </p>
            <div class="column">
              <video id="result3" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/result3.mp4"
                        type="video/mp4">
              </video>
            </div>
            <p>
              We then evaluate the policy performance when adding some disturbances in the experimental environment. The results demonstrate that our in-the-wild learning framework can leverage diverse in-the-wild demonstrations to make the learned policy more robust and generalizable to various environmental disturbances.
            </p>

          <p>
            <details>
              <summary style="color:slateblue">&nbsp;<b>Links to  full videos in this experiment.</b></summary>
              <br>
            <table style="text-align:center">
              <tr><td><b># Teleoperated Demonstrations</b></td> <td><b># In-the-Wild Demonstrations</b></td><td><b>Method</b></td> <td><b>Video</b></td></tr>
              <tr></tr>  
              <tr>
                <td>10</td><td>-</td><td>ACT</td> <td><a href="https://youtu.be/e9SkaHP4U90">Link</a></td>
              </tr>
              <tr>
                <td>10</td><td>100</td><td>ACT</td> <td><a href="https://youtu.be/cEhb6YDLkgU">Link</a></td>
              </tr>
              </table>
            </details>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
    <h2 class="title">BibTeX</h2>
    </div>
    <pre><code>@article{
    fang2023low,
    title = {ExcavatorRL: A Novel Benchmark and Environment for Reinforcement Learning in Excavation},
    author = {Qianyou Zhao and Duidi Wu and Qikai Xu and Yihao Lei and Qingquan Liu and Lingyu Wanga and Jin Qi and Guoniu Zhu and Jie Hu},
    journal = {arXiv preprint arXiv:2309.14975},
    year = {2024}
}
</code></pre>
  </div>
</section>
-->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. Modified upon original <a href="https://nerfies.github.io/"> Nerfies </a> website (<a href="https://github.com/nerfies/nerfies.github.io">source</a>).
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
